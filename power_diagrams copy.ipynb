{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import ot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "Smooth and Sparse Optimal Transport solvers (KL an L2 reg.)\n",
    "\n",
    "Implementation of :\n",
    "Smooth and Sparse Optimal Transport.\n",
    "Mathieu Blondel, Vivien Seguy, Antoine Rolet.\n",
    "In Proc. of AISTATS 2018.\n",
    "https://arxiv.org/abs/1710.06276\n",
    "\n",
    "[17] Blondel, M., Seguy, V., & Rolet, A. (2018). Smooth and Sparse Optimal\n",
    "Transport. Proceedings of the Twenty-First International Conference on\n",
    "Artificial Intelligence and Statistics (AISTATS).\n",
    "\n",
    "Original code from https://github.com/mblondel/smooth-ot/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import cupy as np\n",
    "import torch\n",
    "\n",
    "def projection_simplex(V, z=1, axis=None):\n",
    "    \"\"\" Projection of x onto the simplex, scaled by z\n",
    "\n",
    "        P(x; z) = argmin_{y >= 0, sum(y) = z} ||y - x||^2\n",
    "    z: float or array\n",
    "        If array, len(z) must be compatible with V\n",
    "    axis: None or int\n",
    "        - axis=None: project V by P(V.ravel(); z)\n",
    "        - axis=1: project each V[i] by P(V[i]; z[i])\n",
    "        - axis=0: project each V[:, j] by P(V[:, j]; z[j])\n",
    "    \"\"\"\n",
    "    if axis == 1:\n",
    "        n_features = V.shape[1]\n",
    "        U = torch.sort(V, axis=1)[:, ::-1]\n",
    "        z = torch.ones(len(V)) * z\n",
    "        cssv = torch.cumsum(U, axis=1) - z[:, np.newaxis]\n",
    "        ind = np.arange(n_features) + 1\n",
    "        cond = U - cssv / ind > 0\n",
    "        rho = np.count_nonzero(cond, axis=1)\n",
    "        theta = cssv[np.arange(len(V)), rho - 1] / rho\n",
    "        return np.maximum(V - theta[:, np.newaxis], 0)\n",
    "\n",
    "    elif axis == 0:\n",
    "        return projection_simplex(V.T, z, axis=1).T\n",
    "\n",
    "    else:\n",
    "        V = V.ravel().reshape(1, -1)\n",
    "        return projection_simplex(V, z, axis=1).ravel()\n",
    "\n",
    "\n",
    "def projection_simplex(V, z=1, axis=None):\n",
    "    \"\"\" Projection of x onto the simplex, scaled by z\n",
    "\n",
    "        P(x; z) = argmin_{y >= 0, sum(y) = z} ||y - x||^2\n",
    "    z: float or array\n",
    "        If array, len(z) must be compatible with V\n",
    "    axis: None or int\n",
    "        - axis=None: project V by P(V.ravel(); z)\n",
    "        - axis=1: project each V[i] by P(V[i]; z[i])\n",
    "        - axis=0: project each V[:, j] by P(V[:, j]; z[j])\n",
    "    \"\"\"\n",
    "    if axis == 1:\n",
    "        n_features = V.shape[1]\n",
    "        U = np.sort(V, axis=1)[:, ::-1]\n",
    "        z = np.ones(len(V)) * z\n",
    "        cssv = np.cumsum(U, axis=1) - z[:, np.newaxis]\n",
    "        ind = np.arange(n_features) + 1\n",
    "        cond = U - cssv / ind > 0\n",
    "        rho = np.count_nonzero(cond, axis=1)\n",
    "        theta = cssv[np.arange(len(V)), rho - 1] / rho\n",
    "        return np.maximum(V - theta[:, np.newaxis], 0)\n",
    "\n",
    "    elif axis == 0:\n",
    "        return projection_simplex(V.T, z, axis=1).T\n",
    "\n",
    "    else:\n",
    "        V = V.ravel().reshape(1, -1)\n",
    "        return projection_simplex(V, z, axis=1).ravel()\n",
    "\n",
    "class Regularization(object):\n",
    "    \"\"\"Base class for Regularization objects\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This class is not intended for direct use but as aparent for true\n",
    "        regularizatiojn implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma: float\n",
    "            Regularization parameter.\n",
    "            We recover unregularized OT when gamma -> 0.\n",
    "\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def delta_Omega(X):\n",
    "        \"\"\"\n",
    "        Compute delta_Omega(X[:, j]) for each X[:, j].\n",
    "        delta_Omega(x) = sup_{y >= 0} y^T x - Omega(y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape = len(a) x len(b)\n",
    "            Input array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        v: array, len(b)\n",
    "            Values: v[j] = delta_Omega(X[:, j])\n",
    "        G: array, len(a) x len(b)\n",
    "            Gradients: G[:, j] = nabla delta_Omega(X[:, j])\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def max_Omega(X, b):\n",
    "        \"\"\"\n",
    "        Compute max_Omega_j(X[:, j]) for each X[:, j].\n",
    "        max_Omega_j(x) = sup_{y >= 0, sum(y) = 1} y^T x - Omega(b[j] y) / b[j].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape = len(a) x len(b)\n",
    "            Input array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        v: array, len(b)\n",
    "            Values: v[j] = max_Omega_j(X[:, j])\n",
    "        G: array, len(a) x len(b)\n",
    "            Gradients: G[:, j] = nabla max_Omega_j(X[:, j])\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def Omega(T):\n",
    "        \"\"\"\n",
    "        Compute regularization term.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T: array, shape = len(a) x len(b)\n",
    "            Input array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        value: float\n",
    "            Regularization term.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class NegEntropy(Regularization):\n",
    "    \"\"\" NegEntropy regularization \"\"\"\n",
    "\n",
    "    def delta_Omega(self, X):\n",
    "        G = np.exp(X / self.gamma - 1)\n",
    "        val = self.gamma * np.sum(G, axis=0)\n",
    "        return val, G\n",
    "\n",
    "    def max_Omega(self, X, b):\n",
    "        max_X = np.max(X, axis=0) / self.gamma\n",
    "        exp_X = np.exp(X / self.gamma - max_X)\n",
    "        val = self.gamma * (np.log(np.sum(exp_X, axis=0)) + max_X)\n",
    "        val -= self.gamma * np.log(b)\n",
    "        G = exp_X / np.sum(exp_X, axis=0)\n",
    "        return val, G\n",
    "\n",
    "    def Omega(self, T):\n",
    "        return self.gamma * np.sum(T * np.log(T))\n",
    "\n",
    "\n",
    "class SquaredL2(Regularization):\n",
    "    \"\"\" Squared L2 regularization \"\"\"\n",
    "\n",
    "    def delta_Omega(self, X):\n",
    "        max_X = np.maximum(X, 0)\n",
    "        val = np.sum(max_X ** 2, axis=0) / (2 * self.gamma)\n",
    "        G = max_X / self.gamma\n",
    "        return val, G\n",
    "\n",
    "    def max_Omega(self, X, b):\n",
    "        G = projection_simplex(X / (b * self.gamma), axis=0)\n",
    "        val = np.sum(X * G, axis=0)\n",
    "        val -= 0.5 * self.gamma * b * np.sum(G * G, axis=0)\n",
    "        return val, G\n",
    "\n",
    "    def Omega(self, T):\n",
    "        return 0.5 * self.gamma * np.sum(T ** 2)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62fe073c9e52f36e0693495c8966817261ae99bc6dbb9478924c621f3f898b2e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('sd': conda)",
   "language": "python",
   "name": "python395jvsc74a57bd062fe073c9e52f36e0693495c8966817261ae99bc6dbb9478924c621f3f898b2e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}